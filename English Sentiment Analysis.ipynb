{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "English Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBj6zWYy8UQ3",
        "colab_type": "text"
      },
      "source": [
        "credit : [@nagarindkx](https://github.com/nagarindkx) ดู[โค้ดเดิม](https://github.com/nagarindkx/python/blob/master/python%2006%20Sentiment%20Analysis.ipynb) และอ่าน[บทความ](https://sysadmin.psu.ac.th/2019/01/15/python-06-sentiment-analysis-with-keras-tensorflow/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3zPfA4-alDJ",
        "colab_type": "text"
      },
      "source": [
        "## Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIkdRgYp8UQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = {0:'Negative', 1:'Neutral', 2:'Positive'}\n",
        "negative = [\n",
        "    ['I do not like it', 0],\n",
        "    ['bad movie', 0],\n",
        "    ['I hate it', 0],\n",
        "    ['Not good at all', 0]\n",
        "]\n",
        "neutral = [\n",
        "    ['not bad', 1],\n",
        "    ['so so', 1],\n",
        "    ['OK', 1],\n",
        "    ['no comment', 1]\n",
        "]\n",
        "positive = [\n",
        "    ['Good movie', 2],\n",
        "    ['I love it', 2],\n",
        "    ['Like', 2],\n",
        "    ['Two thumbs up', 2]\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uezcDFKx9NYk",
        "colab_type": "code",
        "outputId": "1711fd02-6d00-4e58-f16a-1d0ad6eaf102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWh7odZp8UQ9",
        "colab_type": "code",
        "outputId": "bb039115-f947-462c-9e33-9d15fe8e29e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "data = np.array(negative + neutral + positive)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['bad movie', '0'],\n",
              "       ['I hate it', '0'],\n",
              "       ['I do not like it', '0'],\n",
              "       ['Not good at all', '0'],\n",
              "       ['not bad', '1'],\n",
              "       ['so so', '1'],\n",
              "       ['OK', '1'],\n",
              "       ['no comment', '1'],\n",
              "       ['Good movie', '2'],\n",
              "       ['I love it', '2'],\n",
              "       ['Like', '2'],\n",
              "       ['Two thumbs up', '2']], dtype='<U16')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f9WqQIb8URB",
        "colab_type": "code",
        "outputId": "2d89adb7-ed7c-4fe6-c6cf-36321f914662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data[:,0])\n",
        "tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'all': 12,\n",
              " 'at': 11,\n",
              " 'bad': 4,\n",
              " 'comment': 15,\n",
              " 'do': 10,\n",
              " 'good': 7,\n",
              " 'hate': 9,\n",
              " 'i': 1,\n",
              " 'it': 2,\n",
              " 'like': 6,\n",
              " 'love': 16,\n",
              " 'movie': 5,\n",
              " 'no': 14,\n",
              " 'not': 3,\n",
              " 'ok': 13,\n",
              " 'so': 8,\n",
              " 'thumbs': 18,\n",
              " 'two': 17,\n",
              " 'up': 19}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QT6eNeTB2Q-",
        "colab_type": "code",
        "outputId": "7d5d2c23-95f7-4a70-acc8-37a90e757090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "tts = tokenizer.texts_to_sequences(data[:,0])\n",
        "tts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 5],\n",
              " [1, 9, 2],\n",
              " [1, 10, 3, 6, 2],\n",
              " [3, 7, 11, 12],\n",
              " [3, 4],\n",
              " [8, 8],\n",
              " [13],\n",
              " [14, 15],\n",
              " [7, 5],\n",
              " [1, 16, 2],\n",
              " [6],\n",
              " [17, 18, 19]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV0Ikna7BZ6V",
        "colab_type": "code",
        "outputId": "e1432213-0d36-4acf-b780-a8742301109d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "maxlen = max([len(s) for s in tts])\n",
        "x = pad_sequences(tts, maxlen=maxlen, padding=\"post\")\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  5,  0,  0,  0],\n",
              "       [ 1,  9,  2,  0,  0],\n",
              "       [ 1, 10,  3,  6,  2],\n",
              "       [ 3,  7, 11, 12,  0],\n",
              "       [ 3,  4,  0,  0,  0],\n",
              "       [ 8,  8,  0,  0,  0],\n",
              "       [13,  0,  0,  0,  0],\n",
              "       [14, 15,  0,  0,  0],\n",
              "       [ 7,  5,  0,  0,  0],\n",
              "       [ 1, 16,  2,  0,  0],\n",
              "       [ 6,  0,  0,  0,  0],\n",
              "       [17, 18, 19,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNR4zV_u8URE",
        "colab_type": "code",
        "outputId": "8b505b53-ab95-4622-8656-d093a9b8370f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "print(\"Unique lables = \" , set(data[:,1]))\n",
        "y = to_categorical([int(i) for i in data[:,1]])\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique lables =  {'1', '2', '0'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQtVmdb6XeL5",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELVq01ZE8URT",
        "colab_type": "code",
        "outputId": "0c50f2ff-7147-47d6-ffd7-2432779da525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# \n",
        "np.random.seed(24)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=maxlen))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(y.shape[1], activation='relu'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x, y, epochs=34)\n",
        "\n",
        "y_predict = model.predict(x)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 5, 10)             200       \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 16)                1728      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 1,979\n",
            "Trainable params: 1,979\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/34\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "12/12 [==============================] - 1s 57ms/step - loss: 4.7406 - acc: 0.3333\n",
            "Epoch 2/34\n",
            "12/12 [==============================] - 0s 553us/step - loss: 3.4613 - acc: 0.3333\n",
            "Epoch 3/34\n",
            "12/12 [==============================] - 0s 623us/step - loss: 1.0841 - acc: 0.4167\n",
            "Epoch 4/34\n",
            "12/12 [==============================] - 0s 564us/step - loss: 0.9540 - acc: 0.3333\n",
            "Epoch 5/34\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.9137 - acc: 0.4167\n",
            "Epoch 6/34\n",
            "12/12 [==============================] - 0s 577us/step - loss: 0.8990 - acc: 0.5833\n",
            "Epoch 7/34\n",
            "12/12 [==============================] - 0s 677us/step - loss: 0.8921 - acc: 0.5000\n",
            "Epoch 8/34\n",
            "12/12 [==============================] - 0s 600us/step - loss: 0.8897 - acc: 0.5000\n",
            "Epoch 9/34\n",
            "12/12 [==============================] - 0s 642us/step - loss: 0.8931 - acc: 0.5000\n",
            "Epoch 10/34\n",
            "12/12 [==============================] - 0s 625us/step - loss: 0.8859 - acc: 0.5000\n",
            "Epoch 11/34\n",
            "12/12 [==============================] - 0s 661us/step - loss: 0.8688 - acc: 0.5000\n",
            "Epoch 12/34\n",
            "12/12 [==============================] - 0s 542us/step - loss: 0.8428 - acc: 0.5000\n",
            "Epoch 13/34\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.8136 - acc: 0.5833\n",
            "Epoch 14/34\n",
            "12/12 [==============================] - 0s 492us/step - loss: 0.7874 - acc: 0.5833\n",
            "Epoch 15/34\n",
            "12/12 [==============================] - 0s 640us/step - loss: 0.7659 - acc: 0.5833\n",
            "Epoch 16/34\n",
            "12/12 [==============================] - 0s 679us/step - loss: 0.7453 - acc: 0.5833\n",
            "Epoch 17/34\n",
            "12/12 [==============================] - 0s 676us/step - loss: 0.7257 - acc: 0.5833\n",
            "Epoch 18/34\n",
            "12/12 [==============================] - 0s 652us/step - loss: 0.7073 - acc: 0.5833\n",
            "Epoch 19/34\n",
            "12/12 [==============================] - 0s 532us/step - loss: 0.6904 - acc: 0.5833\n",
            "Epoch 20/34\n",
            "12/12 [==============================] - 0s 496us/step - loss: 0.6754 - acc: 0.5833\n",
            "Epoch 21/34\n",
            "12/12 [==============================] - 0s 604us/step - loss: 0.6667 - acc: 0.5833\n",
            "Epoch 22/34\n",
            "12/12 [==============================] - 0s 525us/step - loss: 0.6618 - acc: 0.6667\n",
            "Epoch 23/34\n",
            "12/12 [==============================] - 0s 656us/step - loss: 0.6409 - acc: 0.7500\n",
            "Epoch 24/34\n",
            "12/12 [==============================] - 0s 549us/step - loss: 0.6142 - acc: 0.6667\n",
            "Epoch 25/34\n",
            "12/12 [==============================] - 0s 930us/step - loss: 0.5929 - acc: 0.5833\n",
            "Epoch 26/34\n",
            "12/12 [==============================] - 0s 855us/step - loss: 0.5747 - acc: 0.6667\n",
            "Epoch 27/34\n",
            "12/12 [==============================] - 0s 821us/step - loss: 0.5570 - acc: 0.6667\n",
            "Epoch 28/34\n",
            "12/12 [==============================] - 0s 553us/step - loss: 0.5362 - acc: 0.6667\n",
            "Epoch 29/34\n",
            "12/12 [==============================] - 0s 911us/step - loss: 0.5082 - acc: 0.6667\n",
            "Epoch 30/34\n",
            "12/12 [==============================] - 0s 777us/step - loss: 0.4714 - acc: 0.6667\n",
            "Epoch 31/34\n",
            "12/12 [==============================] - 0s 832us/step - loss: 0.4143 - acc: 0.7500\n",
            "Epoch 32/34\n",
            "12/12 [==============================] - 0s 626us/step - loss: 0.3701 - acc: 0.7500\n",
            "Epoch 33/34\n",
            "12/12 [==============================] - 0s 847us/step - loss: 0.3006 - acc: 0.9167\n",
            "Epoch 34/34\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2143 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "garbLWOtXiKC",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48iwFGpJ8URX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=[\n",
        "    ['hate it',0],\n",
        "    ['it is bad',0],\n",
        "    ['do not like',0],\n",
        "    ['just OK',1],\n",
        "    ['so so',1],\n",
        "    ['no idea',1],\n",
        "    ['I do love it',2],\n",
        "    ['Like',2],\n",
        "    ['Thumbs up',2],\n",
        "]\n",
        "data_test = np.array(test)\n",
        "x_data_test = data_test[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szpzFBUk_BL3",
        "colab_type": "code",
        "outputId": "72de75a5-d2f2-4202-beb3-0a86dce43a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test=[]\n",
        "for t in x_data_test:                   \n",
        "    s = []                               # t :                        'I do not like it'\n",
        "    for w in text_to_word_sequence(t):   # text_to_word_sequence(t) : ['I', 'do', 'not', 'like', 'it']\n",
        "        s.append(tokenizer.word_index[w] if w in tokenizer.word_index else 0)\n",
        "    x_test.append(s)\n",
        "x_test = pad_sequences(x_test,maxlen=maxlen, padding='post')\n",
        "\n",
        "model.predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 0, 1, 1, 1, 0, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}
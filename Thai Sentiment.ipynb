{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thai Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AyGJmcJWujZ",
        "colab_type": "text"
      },
      "source": [
        "[ข้อมูล TCAS](https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/tcas61.csv)\n",
        "\n",
        "credit : [@nagarindkx](https://github.com/nagarindkx) ดู[โค้ดเดิม](https://github.com/nagarindkx/python/blob/master/python%2006%20Sentiment%20Analysis.ipynb) และอ่าน[บทความ](https://sysadmin.psu.ac.th/2019/01/15/python-06-sentiment-analysis-with-keras-tensorflow/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiwb5cWoXbVs",
        "colab_type": "code",
        "outputId": "a0887a2e-9755-449a-f055-6dd81b3ba0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "pip install deepcut==0.6.1    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepcut==0.6.1 in /usr/local/lib/python3.6/dist-packages (0.6.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (2.8.0)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (2.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (0.22.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepcut==0.6.1) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepcut==0.6.1) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->deepcut==0.6.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->deepcut==0.6.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->deepcut==0.6.1) (1.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->deepcut==0.6.1) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepcut==0.6.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->deepcut==0.6.1) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7kpDQR9V8uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDff54d-XcB0",
        "colab_type": "code",
        "outputId": "57417b8c-b66d-4d87-fd9c-0c5e7b40bb1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "import deepcut   # 0.6.1 for TensorFlow 1.x\n",
        "deepcut.tokenize('ไปหามเหสี')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ไป', 'หามเหสี']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp4z6Z5HS2dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import *\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spaa93i2S23H",
        "colab_type": "code",
        "outputId": "2c2dae7f-a48f-445d-e998-faf5612c46d9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6950840e-fc65-47c6-8fdd-4f83e84910b3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6950840e-fc65-47c6-8fdd-4f83e84910b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving a.csv to a (8).csv\n",
            "User uploaded file \"a.csv\" with length 34233 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LpjMEhiTsUw",
        "colab_type": "code",
        "outputId": "af5bb5fa-0f00-4e9b-b808-a050bb0f8e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df = pd.read_csv('a (3).csv')\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>มึงกล้าพูดรึป่าวว่าระบบการศึกษามันดีอ่ะ ถุ้ยเฟ...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>เบื่อเวลามาโพสตไรแบบนี้ชอบเป็นพวกที่ใช่โปรไฟล์...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>พ่อมึงเป็นติ่งรัฐบาลหรอสัส ที่เรียกเก็บตังแพงม...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ใครก็ช่วยลบไอ้นี้ออกจากกลุ่มหน่อยครับ มันมาโพส...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>เครียดมากอะตอนนี้</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  มึงกล้าพูดรึป่าวว่าระบบการศึกษามันดีอ่ะ ถุ้ยเฟ...   neg\n",
              "1  เบื่อเวลามาโพสตไรแบบนี้ชอบเป็นพวกที่ใช่โปรไฟล์...   neg\n",
              "2  พ่อมึงเป็นติ่งรัฐบาลหรอสัส ที่เรียกเก็บตังแพงม...   neg\n",
              "3  ใครก็ช่วยลบไอ้นี้ออกจากกลุ่มหน่อยครับ มันมาโพส...   neg\n",
              "4                                  เครียดมากอะตอนนี้   neg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5hHXkGsT1Qn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "237a7f40-533e-4abd-f2c1-1cab5f210012"
      },
      "source": [
        "t = df['text'].apply(lambda s : deepcut.tokenize(s))\n",
        "t.iloc[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['มึง',\n",
              " 'กล้า',\n",
              " 'พูด',\n",
              " 'รึ',\n",
              " 'ป่าว',\n",
              " 'ว่า',\n",
              " 'ระบบ',\n",
              " 'การ',\n",
              " 'ศึกษา',\n",
              " 'มัน',\n",
              " 'ดี',\n",
              " 'อ่ะ',\n",
              " ' ',\n",
              " 'ถุ้ยเฟส',\n",
              " 'ยัง',\n",
              " 'ปลอม',\n",
              " 'เลย',\n",
              " 'สัส']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za0R6vY5We6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae6ae6b2-7556-4042-adc1-798ffe22155d"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(t)\n",
        "len(tokenizer.word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "802"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRvs8XgJYfZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e33f9e9d-ebfc-41d9-bcf5-e036002b1934"
      },
      "source": [
        "tts = tokenizer.texts_to_sequences(t)\n",
        "tts[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[95, 188, 124, 189, 350, 23, 57, 53, 125, 34, 58, 44, 1, 351, 35, 352, 20, 190]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt_wvYStcS29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "40c4e307-68c8-4b5b-9080-bb3a1f588e1d"
      },
      "source": [
        "maxlen = max([len(s) for s in tts])\n",
        "print(maxlen)\n",
        "x = pad_sequences(tts, maxlen=maxlen, padding=\"post\")\n",
        "x"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 95, 188, 124, ...,   0,   0,   0],\n",
              "       [154,  59,  15, ...,   0,   0,   0],\n",
              "       [193,  95,  24, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [228,  91,   1, ...,   0,   0,   0],\n",
              "       [228,  91, 140, ...,   0,   0,   0],\n",
              "       [151,  99,   9, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Z1EdbwiGU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f009c65-fb0d-46c1-cc12-f50078ad320c"
      },
      "source": [
        "maxlen = 50\n",
        "x = x[:, :maxlen]\n",
        "x.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q0lhoS4aM9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "8c68299e-bccf-45fd-d9df-d103c8e46f6f"
      },
      "source": [
        "y = to_categorical([s for s in df['label'].apply(lambda s : 0 if s == 'neg' else 1)])\n",
        "y[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acqbZMxEc6cd",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQAa1eNrjuiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df1ebbb3-fe51-4a65-9c65-e910d89a4cfe"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('vocab size', vocab_size)\n",
        "\n",
        "np.random.seed(8)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=maxlen))\n",
        "model.add(GRU(4))\n",
        "model.add(Dense(y.shape[1], activation='relu'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "h = model.fit(x, y, epochs=300, verbose=1)\n",
        "print(s, max(h.history['acc']), h.history['acc'][-1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size 803\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/300\n",
            "123/123 [==============================] - 1s 8ms/step - loss: 6.8898 - acc: 0.5000\n",
            "Epoch 2/300\n",
            "123/123 [==============================] - 0s 927us/step - loss: 4.2133 - acc: 0.5000\n",
            "Epoch 3/300\n",
            "123/123 [==============================] - 0s 974us/step - loss: 3.8569 - acc: 0.5000\n",
            "Epoch 4/300\n",
            "123/123 [==============================] - 0s 989us/step - loss: 3.6894 - acc: 0.5000\n",
            "Epoch 5/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 2.2933 - acc: 0.5000\n",
            "Epoch 6/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.5754 - acc: 0.5000\n",
            "Epoch 7/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.4350 - acc: 0.5000\n",
            "Epoch 8/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.3537 - acc: 0.5000\n",
            "Epoch 9/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.2926 - acc: 0.5000\n",
            "Epoch 10/300\n",
            "123/123 [==============================] - 0s 970us/step - loss: 1.2481 - acc: 0.5000\n",
            "Epoch 11/300\n",
            "123/123 [==============================] - 0s 970us/step - loss: 1.2127 - acc: 0.5000\n",
            "Epoch 12/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.1817 - acc: 0.5000\n",
            "Epoch 13/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.1554 - acc: 0.5000\n",
            "Epoch 14/300\n",
            "123/123 [==============================] - 0s 980us/step - loss: 1.1316 - acc: 0.5000\n",
            "Epoch 15/300\n",
            "123/123 [==============================] - 0s 986us/step - loss: 1.1098 - acc: 0.5000\n",
            "Epoch 16/300\n",
            "123/123 [==============================] - 0s 976us/step - loss: 1.0896 - acc: 0.5000\n",
            "Epoch 17/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0705 - acc: 0.5000\n",
            "Epoch 18/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0524 - acc: 0.5000\n",
            "Epoch 19/300\n",
            "123/123 [==============================] - 0s 940us/step - loss: 1.0350 - acc: 0.5000\n",
            "Epoch 20/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0183 - acc: 0.5000\n",
            "Epoch 21/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0022 - acc: 0.5000\n",
            "Epoch 22/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.9865 - acc: 0.5000\n",
            "Epoch 23/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.9712 - acc: 0.5000\n",
            "Epoch 24/300\n",
            "123/123 [==============================] - 0s 994us/step - loss: 0.9564 - acc: 0.5000\n",
            "Epoch 25/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.9420 - acc: 0.5000\n",
            "Epoch 26/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.9279 - acc: 0.5000\n",
            "Epoch 27/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.9141 - acc: 0.5000\n",
            "Epoch 28/300\n",
            "123/123 [==============================] - 0s 949us/step - loss: 0.9006 - acc: 0.5000\n",
            "Epoch 29/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.8875 - acc: 0.5000\n",
            "Epoch 30/300\n",
            "123/123 [==============================] - 0s 982us/step - loss: 0.8747 - acc: 0.5000\n",
            "Epoch 31/300\n",
            "123/123 [==============================] - 0s 986us/step - loss: 0.8622 - acc: 0.5000\n",
            "Epoch 32/300\n",
            "123/123 [==============================] - 0s 996us/step - loss: 0.8499 - acc: 0.5000\n",
            "Epoch 33/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.8380 - acc: 0.5000\n",
            "Epoch 34/300\n",
            "123/123 [==============================] - 0s 944us/step - loss: 0.8263 - acc: 0.5000\n",
            "Epoch 35/300\n",
            "123/123 [==============================] - 0s 982us/step - loss: 0.8150 - acc: 0.5000\n",
            "Epoch 36/300\n",
            "123/123 [==============================] - 0s 987us/step - loss: 0.8039 - acc: 0.5000\n",
            "Epoch 37/300\n",
            "123/123 [==============================] - 0s 948us/step - loss: 0.7931 - acc: 0.5000\n",
            "Epoch 38/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7826 - acc: 0.5000\n",
            "Epoch 39/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7725 - acc: 0.5000\n",
            "Epoch 40/300\n",
            "123/123 [==============================] - 0s 973us/step - loss: 0.7626 - acc: 0.5000\n",
            "Epoch 41/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7531 - acc: 0.5000\n",
            "Epoch 42/300\n",
            "123/123 [==============================] - 0s 955us/step - loss: 0.7440 - acc: 0.5000\n",
            "Epoch 43/300\n",
            "123/123 [==============================] - 0s 977us/step - loss: 0.7353 - acc: 0.5000\n",
            "Epoch 44/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7267 - acc: 0.5000\n",
            "Epoch 45/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7187 - acc: 0.5000\n",
            "Epoch 46/300\n",
            "123/123 [==============================] - 0s 980us/step - loss: 0.7112 - acc: 0.5000\n",
            "Epoch 47/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.7041 - acc: 0.5000\n",
            "Epoch 48/300\n",
            "123/123 [==============================] - 0s 990us/step - loss: 0.6973 - acc: 0.5000\n",
            "Epoch 49/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6913 - acc: 0.5000\n",
            "Epoch 50/300\n",
            "123/123 [==============================] - 0s 987us/step - loss: 0.6853 - acc: 0.5000\n",
            "Epoch 51/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6803 - acc: 0.5000\n",
            "Epoch 52/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6755 - acc: 0.5041\n",
            "Epoch 53/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6714 - acc: 0.5732\n",
            "Epoch 54/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6679 - acc: 0.5894\n",
            "Epoch 55/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6647 - acc: 0.5935\n",
            "Epoch 56/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6620 - acc: 0.6057\n",
            "Epoch 57/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6597 - acc: 0.6138\n",
            "Epoch 58/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6581 - acc: 0.6260\n",
            "Epoch 59/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6568 - acc: 0.6260\n",
            "Epoch 60/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6551 - acc: 0.6341\n",
            "Epoch 61/300\n",
            "123/123 [==============================] - 0s 954us/step - loss: 0.6543 - acc: 0.6382\n",
            "Epoch 62/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6534 - acc: 0.6423\n",
            "Epoch 63/300\n",
            "123/123 [==============================] - 0s 961us/step - loss: 0.6527 - acc: 0.6423\n",
            "Epoch 64/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6520 - acc: 0.6423\n",
            "Epoch 65/300\n",
            "123/123 [==============================] - 0s 998us/step - loss: 0.6514 - acc: 0.6463\n",
            "Epoch 66/300\n",
            "123/123 [==============================] - 0s 980us/step - loss: 0.6505 - acc: 0.6545\n",
            "Epoch 67/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6499 - acc: 0.6545\n",
            "Epoch 68/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6495 - acc: 0.6545\n",
            "Epoch 69/300\n",
            "123/123 [==============================] - 0s 967us/step - loss: 0.6487 - acc: 0.6545\n",
            "Epoch 70/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6481 - acc: 0.6545\n",
            "Epoch 71/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6475 - acc: 0.6545\n",
            "Epoch 72/300\n",
            "123/123 [==============================] - 0s 946us/step - loss: 0.6469 - acc: 0.6545\n",
            "Epoch 73/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6464 - acc: 0.6545\n",
            "Epoch 74/300\n",
            "123/123 [==============================] - 0s 990us/step - loss: 0.6458 - acc: 0.6545\n",
            "Epoch 75/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6452 - acc: 0.6545\n",
            "Epoch 76/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6447 - acc: 0.6545\n",
            "Epoch 77/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6441 - acc: 0.6545\n",
            "Epoch 78/300\n",
            "123/123 [==============================] - 0s 977us/step - loss: 0.6436 - acc: 0.6545\n",
            "Epoch 79/300\n",
            "123/123 [==============================] - 0s 963us/step - loss: 0.6430 - acc: 0.6545\n",
            "Epoch 80/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6426 - acc: 0.6545\n",
            "Epoch 81/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6421 - acc: 0.6545\n",
            "Epoch 82/300\n",
            "123/123 [==============================] - 0s 916us/step - loss: 0.6416 - acc: 0.6545\n",
            "Epoch 83/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6410 - acc: 0.6545\n",
            "Epoch 84/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6405 - acc: 0.6545\n",
            "Epoch 85/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6400 - acc: 0.6545\n",
            "Epoch 86/300\n",
            "123/123 [==============================] - 0s 962us/step - loss: 0.6395 - acc: 0.6545\n",
            "Epoch 87/300\n",
            "123/123 [==============================] - 0s 930us/step - loss: 0.6390 - acc: 0.6545\n",
            "Epoch 88/300\n",
            "123/123 [==============================] - 0s 983us/step - loss: 0.6385 - acc: 0.6545\n",
            "Epoch 89/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6380 - acc: 0.6545\n",
            "Epoch 90/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6375 - acc: 0.6545\n",
            "Epoch 91/300\n",
            "123/123 [==============================] - 0s 918us/step - loss: 0.6370 - acc: 0.6545\n",
            "Epoch 92/300\n",
            "123/123 [==============================] - 0s 970us/step - loss: 0.6366 - acc: 0.6545\n",
            "Epoch 93/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6361 - acc: 0.6545\n",
            "Epoch 94/300\n",
            "123/123 [==============================] - 0s 979us/step - loss: 0.6355 - acc: 0.6585\n",
            "Epoch 95/300\n",
            "123/123 [==============================] - 0s 966us/step - loss: 0.6351 - acc: 0.6585\n",
            "Epoch 96/300\n",
            "123/123 [==============================] - 0s 925us/step - loss: 0.6345 - acc: 0.6585\n",
            "Epoch 97/300\n",
            "123/123 [==============================] - 0s 961us/step - loss: 0.6341 - acc: 0.6585\n",
            "Epoch 98/300\n",
            "123/123 [==============================] - 0s 934us/step - loss: 0.6335 - acc: 0.6585\n",
            "Epoch 99/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6331 - acc: 0.6585\n",
            "Epoch 100/300\n",
            "123/123 [==============================] - 0s 934us/step - loss: 0.6326 - acc: 0.6585\n",
            "Epoch 101/300\n",
            "123/123 [==============================] - 0s 942us/step - loss: 0.6321 - acc: 0.6585\n",
            "Epoch 102/300\n",
            "123/123 [==============================] - 0s 941us/step - loss: 0.6315 - acc: 0.6585\n",
            "Epoch 103/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6310 - acc: 0.6585\n",
            "Epoch 104/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6306 - acc: 0.6585\n",
            "Epoch 105/300\n",
            "123/123 [==============================] - 0s 974us/step - loss: 0.6299 - acc: 0.6585\n",
            "Epoch 106/300\n",
            "123/123 [==============================] - 0s 993us/step - loss: 0.6295 - acc: 0.6585\n",
            "Epoch 107/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6290 - acc: 0.6585\n",
            "Epoch 108/300\n",
            "123/123 [==============================] - 0s 999us/step - loss: 0.6283 - acc: 0.6585\n",
            "Epoch 109/300\n",
            "123/123 [==============================] - 0s 988us/step - loss: 0.6278 - acc: 0.6585\n",
            "Epoch 110/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6272 - acc: 0.6585\n",
            "Epoch 111/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6266 - acc: 0.6585\n",
            "Epoch 112/300\n",
            "123/123 [==============================] - 0s 994us/step - loss: 0.6261 - acc: 0.6585\n",
            "Epoch 113/300\n",
            "123/123 [==============================] - 0s 956us/step - loss: 0.6255 - acc: 0.6585\n",
            "Epoch 114/300\n",
            "123/123 [==============================] - 0s 949us/step - loss: 0.6248 - acc: 0.6585\n",
            "Epoch 115/300\n",
            "123/123 [==============================] - 0s 958us/step - loss: 0.6243 - acc: 0.6585\n",
            "Epoch 116/300\n",
            "123/123 [==============================] - 0s 961us/step - loss: 0.6237 - acc: 0.6585\n",
            "Epoch 117/300\n",
            "123/123 [==============================] - 0s 977us/step - loss: 0.6229 - acc: 0.6585\n",
            "Epoch 118/300\n",
            "123/123 [==============================] - 0s 913us/step - loss: 0.6223 - acc: 0.6585\n",
            "Epoch 119/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6217 - acc: 0.6585\n",
            "Epoch 120/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6210 - acc: 0.6585\n",
            "Epoch 121/300\n",
            "123/123 [==============================] - 0s 956us/step - loss: 0.6202 - acc: 0.6585\n",
            "Epoch 122/300\n",
            "123/123 [==============================] - 0s 967us/step - loss: 0.6195 - acc: 0.6585\n",
            "Epoch 123/300\n",
            "123/123 [==============================] - 0s 987us/step - loss: 0.6188 - acc: 0.6585\n",
            "Epoch 124/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6180 - acc: 0.6585\n",
            "Epoch 125/300\n",
            "123/123 [==============================] - 0s 920us/step - loss: 0.6172 - acc: 0.6585\n",
            "Epoch 126/300\n",
            "123/123 [==============================] - 0s 925us/step - loss: 0.6165 - acc: 0.6585\n",
            "Epoch 127/300\n",
            "123/123 [==============================] - 0s 981us/step - loss: 0.6157 - acc: 0.6585\n",
            "Epoch 128/300\n",
            "123/123 [==============================] - 0s 944us/step - loss: 0.6150 - acc: 0.6585\n",
            "Epoch 129/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6141 - acc: 0.6585\n",
            "Epoch 130/300\n",
            "123/123 [==============================] - 0s 967us/step - loss: 0.6132 - acc: 0.6585\n",
            "Epoch 131/300\n",
            "123/123 [==============================] - 0s 944us/step - loss: 0.6124 - acc: 0.6585\n",
            "Epoch 132/300\n",
            "123/123 [==============================] - 0s 997us/step - loss: 0.6115 - acc: 0.6585\n",
            "Epoch 133/300\n",
            "123/123 [==============================] - 0s 951us/step - loss: 0.6106 - acc: 0.6585\n",
            "Epoch 134/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6096 - acc: 0.6585\n",
            "Epoch 135/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6087 - acc: 0.6585\n",
            "Epoch 136/300\n",
            "123/123 [==============================] - 0s 983us/step - loss: 0.6077 - acc: 0.6585\n",
            "Epoch 137/300\n",
            "123/123 [==============================] - 0s 967us/step - loss: 0.6069 - acc: 0.6585\n",
            "Epoch 138/300\n",
            "123/123 [==============================] - 0s 984us/step - loss: 0.6057 - acc: 0.6585\n",
            "Epoch 139/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6047 - acc: 0.6585\n",
            "Epoch 140/300\n",
            "123/123 [==============================] - 0s 995us/step - loss: 0.6036 - acc: 0.6585\n",
            "Epoch 141/300\n",
            "123/123 [==============================] - 0s 983us/step - loss: 0.6025 - acc: 0.6585\n",
            "Epoch 142/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6016 - acc: 0.6585\n",
            "Epoch 143/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.6005 - acc: 0.6626\n",
            "Epoch 144/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5994 - acc: 0.6626\n",
            "Epoch 145/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5983 - acc: 0.6626\n",
            "Epoch 146/300\n",
            "123/123 [==============================] - 0s 1000us/step - loss: 0.5971 - acc: 0.6626\n",
            "Epoch 147/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5959 - acc: 0.6626\n",
            "Epoch 148/300\n",
            "123/123 [==============================] - 0s 990us/step - loss: 0.5948 - acc: 0.6626\n",
            "Epoch 149/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5934 - acc: 0.6626\n",
            "Epoch 150/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5921 - acc: 0.6626\n",
            "Epoch 151/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5905 - acc: 0.6667\n",
            "Epoch 152/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5891 - acc: 0.6667\n",
            "Epoch 153/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5875 - acc: 0.6667\n",
            "Epoch 154/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5859 - acc: 0.6667\n",
            "Epoch 155/300\n",
            "123/123 [==============================] - 0s 981us/step - loss: 0.5841 - acc: 0.6667\n",
            "Epoch 156/300\n",
            "123/123 [==============================] - 0s 970us/step - loss: 0.5823 - acc: 0.6667\n",
            "Epoch 157/300\n",
            "123/123 [==============================] - 0s 990us/step - loss: 0.5802 - acc: 0.6667\n",
            "Epoch 158/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5780 - acc: 0.6667\n",
            "Epoch 159/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5757 - acc: 0.6707\n",
            "Epoch 160/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5728 - acc: 0.6707\n",
            "Epoch 161/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5702 - acc: 0.6707\n",
            "Epoch 162/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5669 - acc: 0.6707\n",
            "Epoch 163/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5629 - acc: 0.6707\n",
            "Epoch 164/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5591 - acc: 0.6707\n",
            "Epoch 165/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5550 - acc: 0.6707\n",
            "Epoch 166/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5510 - acc: 0.6707\n",
            "Epoch 167/300\n",
            "123/123 [==============================] - 0s 973us/step - loss: 0.5462 - acc: 0.6748\n",
            "Epoch 168/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5407 - acc: 0.6789\n",
            "Epoch 169/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5346 - acc: 0.6829\n",
            "Epoch 170/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5279 - acc: 0.6829\n",
            "Epoch 171/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5212 - acc: 0.6829\n",
            "Epoch 172/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5137 - acc: 0.6829\n",
            "Epoch 173/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.5064 - acc: 0.6870\n",
            "Epoch 174/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4985 - acc: 0.6870\n",
            "Epoch 175/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4907 - acc: 0.6911\n",
            "Epoch 176/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4823 - acc: 0.6911\n",
            "Epoch 177/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4735 - acc: 0.6951\n",
            "Epoch 178/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4658 - acc: 0.6992\n",
            "Epoch 179/300\n",
            "123/123 [==============================] - 0s 994us/step - loss: 0.4579 - acc: 0.7033\n",
            "Epoch 180/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4501 - acc: 0.7358\n",
            "Epoch 181/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4424 - acc: 0.7764\n",
            "Epoch 182/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4343 - acc: 0.8130\n",
            "Epoch 183/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4264 - acc: 0.8089\n",
            "Epoch 184/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4178 - acc: 0.8089\n",
            "Epoch 185/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4093 - acc: 0.8089\n",
            "Epoch 186/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.4001 - acc: 0.8089\n",
            "Epoch 187/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3902 - acc: 0.8089\n",
            "Epoch 188/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3816 - acc: 0.8130\n",
            "Epoch 189/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3727 - acc: 0.8130\n",
            "Epoch 190/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3632 - acc: 0.8130\n",
            "Epoch 191/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3534 - acc: 0.8130\n",
            "Epoch 192/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3446 - acc: 0.8130\n",
            "Epoch 193/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3344 - acc: 0.8130\n",
            "Epoch 194/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3245 - acc: 0.8130\n",
            "Epoch 195/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3141 - acc: 0.8130\n",
            "Epoch 196/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.3038 - acc: 0.8130\n",
            "Epoch 197/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2929 - acc: 0.8171\n",
            "Epoch 198/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2812 - acc: 0.8211\n",
            "Epoch 199/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2692 - acc: 0.8211\n",
            "Epoch 200/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2557 - acc: 0.8211\n",
            "Epoch 201/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2424 - acc: 0.8252\n",
            "Epoch 202/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2279 - acc: 0.8333\n",
            "Epoch 203/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.2128 - acc: 0.8415\n",
            "Epoch 204/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.1961 - acc: 0.8699\n",
            "Epoch 205/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.1810 - acc: 0.9431\n",
            "Epoch 206/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.1645 - acc: 0.9959\n",
            "Epoch 207/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.1422 - acc: 1.0000\n",
            "Epoch 208/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.1203 - acc: 1.0000\n",
            "Epoch 209/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0945 - acc: 1.0000\n",
            "Epoch 210/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0618 - acc: 1.0000\n",
            "Epoch 211/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0386 - acc: 1.0000\n",
            "Epoch 212/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0285 - acc: 1.0000\n",
            "Epoch 213/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0228 - acc: 1.0000\n",
            "Epoch 214/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0187 - acc: 1.0000\n",
            "Epoch 215/300\n",
            "123/123 [==============================] - 0s 987us/step - loss: 0.0148 - acc: 1.0000\n",
            "Epoch 216/300\n",
            "123/123 [==============================] - 0s 981us/step - loss: 0.0122 - acc: 1.0000\n",
            "Epoch 217/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 1.0000\n",
            "Epoch 218/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 219/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 220/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 221/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 222/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 223/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 224/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 225/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 226/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 227/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 228/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 229/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 230/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 231/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 232/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 233/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 234/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 9.8375e-04 - acc: 1.0000\n",
            "Epoch 235/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 8.5431e-04 - acc: 1.0000\n",
            "Epoch 236/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 7.1012e-04 - acc: 1.0000\n",
            "Epoch 237/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 5.9248e-04 - acc: 1.0000\n",
            "Epoch 238/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 5.3237e-04 - acc: 1.0000\n",
            "Epoch 239/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 4.5166e-04 - acc: 1.0000\n",
            "Epoch 240/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 3.6927e-04 - acc: 1.0000\n",
            "Epoch 241/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 3.2737e-04 - acc: 1.0000\n",
            "Epoch 242/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 2.8296e-04 - acc: 1.0000\n",
            "Epoch 243/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 2.3092e-04 - acc: 1.0000\n",
            "Epoch 244/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 2.1569e-04 - acc: 1.0000\n",
            "Epoch 245/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.6316e-04 - acc: 1.0000\n",
            "Epoch 246/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.5297e-04 - acc: 1.0000\n",
            "Epoch 247/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.1184e-04 - acc: 1.0000\n",
            "Epoch 248/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 9.4125e-05 - acc: 1.0000\n",
            "Epoch 249/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 8.6561e-05 - acc: 1.0000\n",
            "Epoch 250/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 7.4258e-05 - acc: 1.0000\n",
            "Epoch 251/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 6.6554e-05 - acc: 1.0000\n",
            "Epoch 252/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 6.1103e-05 - acc: 1.0000\n",
            "Epoch 253/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 5.4288e-05 - acc: 1.0000\n",
            "Epoch 254/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 4.4652e-05 - acc: 1.0000\n",
            "Epoch 255/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 3.5279e-05 - acc: 1.0000\n",
            "Epoch 256/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 2.6094e-05 - acc: 1.0000\n",
            "Epoch 257/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.9666e-05 - acc: 1.0000\n",
            "Epoch 258/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.2605e-05 - acc: 1.0000\n",
            "Epoch 259/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 3.0899e-06 - acc: 1.0000\n",
            "Epoch 260/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 261/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 262/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 263/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 8.5663e-07 - acc: 1.0000\n",
            "Epoch 264/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 265/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 266/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 267/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 268/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 269/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 270/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 271/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 272/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 273/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 274/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 275/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 276/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 277/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 278/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 279/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 280/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 281/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 282/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 283/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 284/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 285/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 286/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 287/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 288/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 289/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 290/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 291/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 292/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 293/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 294/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 295/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 296/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 297/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 298/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 299/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 300/300\n",
            "123/123 [==============================] - 0s 1ms/step - loss: 1.0960e-07 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-376086448405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc1p_DcGjZID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97650a0c-1aeb-4bb6-def3-36e478563c19"
      },
      "source": [
        "test=[\n",
        "    'ไม่ติดอ่ะ',\n",
        "    'จน เครียด กินนม',\n",
        "    'นี่แหละคือความกดดัน',\n",
        "    'ทำผมเสียเวลามาก',\n",
        "    'ติดแล้ว',\n",
        "    'ทุกคนสู้ต่อไป',\n",
        "    'ดีใจจัง สอบติดแล้ว',\n",
        "    'จะได้เป็นนิสิตเกษตร',\n",
        "]\n",
        "x_data_test = np.array(test)\n",
        "\n",
        "x_test=[]\n",
        "for t in x_data_test:                   \n",
        "    s = []                          # t :                        'I do not like it'\n",
        "    for w in deepcut.tokenize(t):   # text_to_word_sequence(t) : ['I', 'do', 'not', 'like', 'it']\n",
        "        s.append(tokenizer.word_index[w] if w in tokenizer.word_index else 0)\n",
        "    x_test.append(s)\n",
        "x_test = pad_sequences(x_test,maxlen=50, padding='post')\n",
        "\n",
        "model.predict_classes(x_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dax4FhHsmD4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWziVtUMc9OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "for s in range(7,100):\n",
        "  np.random.seed(s)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=maxlen))\n",
        "  model.add(GRU(4))\n",
        "  model.add(Dense(y.shape[1], activation='relu'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  h = model.fit(x, y, epochs=400, verbose=0)\n",
        "  print(s, max(h.history['acc']), h.history['acc'][-1])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPU3xQE7uYMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}